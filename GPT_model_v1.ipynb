{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMCLVP5CR4hxF+JIzyr3yBz"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pV5Duni6oOzY","executionInfo":{"status":"ok","timestamp":1753259857782,"user_tz":-330,"elapsed":55,"user":{"displayName":"Dayanand K","userId":"10597641448182465017"}},"outputId":"d182f577-faff-44d5-a484-d16fe5ea7b92"},"outputs":[{"output_type":"stream","name":"stdout","text":["I HAD always thought Jack Gisburn rather a cheap g\n"]}],"source":["# loading the book called \"the verdict\"\n","\n","with open('the-verdict.txt', 'r', encoding='utf-8') as f:\n","  raw_text = f.read()\n","\n","print(raw_text[:50])\n"]},{"cell_type":"code","source":["import tiktoken"],"metadata":{"id":"dyNfrVin5NkM","executionInfo":{"status":"ok","timestamp":1753259857993,"user_tz":-330,"elapsed":213,"user":{"displayName":"Dayanand K","userId":"10597641448182465017"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# tokenizer BPE\n","tokenizer = tiktoken.get_encoding('gpt2')"],"metadata":{"id":"GBXc03iO5QwQ","executionInfo":{"status":"ok","timestamp":1753259860675,"user_tz":-330,"elapsed":2687,"user":{"displayName":"Dayanand K","userId":"10597641448182465017"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["total_chars = len(raw_text)\n","\n","total_tokens = len(tokenizer.encode(raw_text))\n","\n","print(f'Total characters: {total_chars}')\n","print(f\"Tokens: {total_tokens}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QpSL2Xce4qUe","executionInfo":{"status":"ok","timestamp":1753259860856,"user_tz":-330,"elapsed":180,"user":{"displayName":"Dayanand K","userId":"10597641448182465017"}},"outputId":"8a13c3a8-5ca5-4168-f2a7-a025ad85b797"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Total characters: 20479\n","Tokens: 5145\n"]}]},{"cell_type":"code","source":["# tokenized_text[:50]"],"metadata":{"id":"qc_-bmHK4-Xc","executionInfo":{"status":"ok","timestamp":1753259860859,"user_tz":-330,"elapsed":2,"user":{"displayName":"Dayanand K","userId":"10597641448182465017"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# implementing Dataset and Dataloader\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","\n","class GPTDataset(Dataset):\n","  def __init__(self, text, tokenizer, max_length, stride):\n","    self.input_ids = []\n","    self.target_ids = []\n","\n","    token_ids = tokenizer.encode(text, allowed_special = {\"<|endoftext|>\"})\n","\n","    for i in range(0, len(token_ids) - max_length, stride):\n","      input_chunk = token_ids[i : i + max_length]\n","      target_chunk = token_ids[i + 1 : i + max_length + 1]\n","\n","      self.input_ids.append(torch.tensor(input_chunk))\n","      self.target_ids.append(torch.tensor(target_chunk))\n","\n","  def __len__(self):\n","    return len(self.input_ids)\n","\n","  def __getitem__(self, idx):\n","    return self.input_ids[idx], self.target_ids[idx]"],"metadata":{"id":"jrZdNTyi6BLY","executionInfo":{"status":"ok","timestamp":1753259866421,"user_tz":-330,"elapsed":5561,"user":{"displayName":"Dayanand K","userId":"10597641448182465017"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# implementing the DataLoader\n","\n","def create_dataloader(txt, batch_size=4, max_length=256, stride = 128,\n","                      shuffle = True, drop_last = True, num_workers = 0):\n","\n","  tokenizer = tiktoken.get_encoding('gpt2')\n","\n","  dataset = GPTDataset(txt, tokenizer, max_length, stride)\n","\n","  dataloader = DataLoader(\n","      dataset,\n","      batch_size = batch_size,\n","      shuffle = shuffle,\n","      drop_last = drop_last,\n","      num_workers = num_workers\n","      )\n","\n","  return dataloader"],"metadata":{"id":"r3V_6ji57TVi","executionInfo":{"status":"ok","timestamp":1753259866442,"user_tz":-330,"elapsed":19,"user":{"displayName":"Dayanand K","userId":"10597641448182465017"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["# GPT model's configuration\n","\n","GPT_CONFIG_124M = {\n","    'vocab_size': 50257,\n","    'context_length': 256,\n","    'emd_dim': 768,\n","    'n_heads': 12,\n","    'n_layers': 12,\n","    'drop_rate': 0.1,\n","    'qkv_bias': False\n","\n","}"],"metadata":{"id":"N8R4dUjK95zO","executionInfo":{"status":"ok","timestamp":1753259866488,"user_tz":-330,"elapsed":14,"user":{"displayName":"Dayanand K","userId":"10597641448182465017"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["# splitting the dataset into train and validation set\n","\n","train_ratio = 0.9\n","split_idx = int(train_ratio * len(raw_text))\n","\n","train_text = raw_text[:split_idx]\n","val_text = raw_text[split_idx:]\n","\n","torch.manual_seed(123)\n","\n","# creating train data loader\n","train_loader = create_dataloader(\n","    train_text,\n","    batch_size = 2,\n","    max_length = GPT_CONFIG_124M['context_length'],\n","    stride = GPT_CONFIG_124M['context_length'],\n","    shuffle = True,\n","    drop_last = True,\n","    num_workers = 0\n",")\n","\n","# creating a validation dataloader\n","\n","val_loader = create_dataloader(\n","    val_text,\n","    batch_size = 2,\n","    max_length = GPT_CONFIG_124M['context_length'],\n","    stride = GPT_CONFIG_124M['context_length'],\n","    shuffle = False,\n","    drop_last = False,\n","    num_workers = 0\n",")"],"metadata":{"id":"cCJJWtIX8c_F","executionInfo":{"status":"ok","timestamp":1753259866491,"user_tz":-330,"elapsed":2,"user":{"displayName":"Dayanand K","userId":"10597641448182465017"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["input, target = train_loader.dataset[0]\n","print(f\" input: {input}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KGIhqY8N9aQJ","executionInfo":{"status":"ok","timestamp":1753259866542,"user_tz":-330,"elapsed":50,"user":{"displayName":"Dayanand K","userId":"10597641448182465017"}},"outputId":"ddf736b0-2a9c-48d9-9653-c5808271bba2"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":[" input: tensor([   40,   367,  2885,  1464,  1807,  3619,   402,   271, 10899,  2138,\n","          257,  7026, 15632,   438,  2016,   257,   922,  5891,  1576,   438,\n","          568,   340,   373,   645,  1049,  5975,   284,   502,   284,  3285,\n","          326,    11,   287,   262,  6001,   286,   465, 13476,    11,   339,\n","          550,  5710,   465, 12036,    11,  6405,   257,  5527, 27075,    11,\n","          290,  4920,  2241,   287,   257,  4489,    64,   319,   262, 34686,\n","        41976,    13,   357, 10915,   314,  2138,  1807,   340,   561,   423,\n","          587, 10598,   393, 28537,  2014,   198,   198,     1,   464,  6001,\n","          286,   465, 13476,     1,   438,  5562,   373,   644,   262,  1466,\n","         1444,   340,    13,   314,   460,  3285,  9074,    13, 46606,   536,\n","         5469,   438, 14363,   938,  4842,  1650,   353,   438,  2934,   489,\n","         3255,   465, 48422,   540,   450,    67,  3299,    13,   366,  5189,\n","         1781,   340,   338,  1016,   284,  3758,   262,  1988,   286,   616,\n","         4286,   705,  1014,   510,    26,   475,   314,   836,   470,   892,\n","          286,   326,    11,  1770,    13,  8759,  2763,   438,  1169,  2994,\n","          284,   943, 17034,   318,   477,   314,   892,   286,   526,   383,\n","         1573,    11,   319,  9074,    13,   536,  5469,   338, 11914,    11,\n","        33096,   663,  4808,  3808,    62,   355,   996,   484,   547, 12548,\n","          287,   281, 13079,   410, 12523,   286, 22353,    13,   843,   340,\n","          373,   407,   691,   262,  9074,    13,   536, 48819,   508, 25722,\n","          276,    13, 11161,   407,   262, 40123, 18113,   544,  9325,   701,\n","           11,   379,   262,   938,   402,  1617,   261, 12917,   905,    11,\n","         5025,   502,   878,   402,   271, 10899,   338,   366, 31640,    12,\n","           67, 20811,     1,   284,   910,    11,   351, 10953,   287,   607,\n","         2951,    25,   366,  1135,  2236,   407,   804,  2402,   663,   588,\n","          757, 13984,   198,   198,  5779, 28112])\n"]}]},{"cell_type":"code","source":["print(f\"target: {target}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9mn4rhNn-sVB","executionInfo":{"status":"ok","timestamp":1753259866669,"user_tz":-330,"elapsed":114,"user":{"displayName":"Dayanand K","userId":"10597641448182465017"}},"outputId":"ab5a237f-95db-476d-8d77-18feae52bc56"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["target: tensor([  367,  2885,  1464,  1807,  3619,   402,   271, 10899,  2138,   257,\n","         7026, 15632,   438,  2016,   257,   922,  5891,  1576,   438,   568,\n","          340,   373,   645,  1049,  5975,   284,   502,   284,  3285,   326,\n","           11,   287,   262,  6001,   286,   465, 13476,    11,   339,   550,\n","         5710,   465, 12036,    11,  6405,   257,  5527, 27075,    11,   290,\n","         4920,  2241,   287,   257,  4489,    64,   319,   262, 34686, 41976,\n","           13,   357, 10915,   314,  2138,  1807,   340,   561,   423,   587,\n","        10598,   393, 28537,  2014,   198,   198,     1,   464,  6001,   286,\n","          465, 13476,     1,   438,  5562,   373,   644,   262,  1466,  1444,\n","          340,    13,   314,   460,  3285,  9074,    13, 46606,   536,  5469,\n","          438, 14363,   938,  4842,  1650,   353,   438,  2934,   489,  3255,\n","          465, 48422,   540,   450,    67,  3299,    13,   366,  5189,  1781,\n","          340,   338,  1016,   284,  3758,   262,  1988,   286,   616,  4286,\n","          705,  1014,   510,    26,   475,   314,   836,   470,   892,   286,\n","          326,    11,  1770,    13,  8759,  2763,   438,  1169,  2994,   284,\n","          943, 17034,   318,   477,   314,   892,   286,   526,   383,  1573,\n","           11,   319,  9074,    13,   536,  5469,   338, 11914,    11, 33096,\n","          663,  4808,  3808,    62,   355,   996,   484,   547, 12548,   287,\n","          281, 13079,   410, 12523,   286, 22353,    13,   843,   340,   373,\n","          407,   691,   262,  9074,    13,   536, 48819,   508, 25722,   276,\n","           13, 11161,   407,   262, 40123, 18113,   544,  9325,   701,    11,\n","          379,   262,   938,   402,  1617,   261, 12917,   905,    11,  5025,\n","          502,   878,   402,   271, 10899,   338,   366, 31640,    12,    67,\n","        20811,     1,   284,   910,    11,   351, 10953,   287,   607,  2951,\n","           25,   366,  1135,  2236,   407,   804,  2402,   663,   588,   757,\n","        13984,   198,   198,  5779, 28112, 10197])\n"]}]},{"cell_type":"code","source":["# sanity check\n","\n","if total_tokens * (train_ratio) < GPT_CONFIG_124M['context_length']:\n","  print(\"Not enough tokens for the training loader, \"\n","  \"Try to lower the GPT_CONFIG_124M['context_length] or \"\n","  \"increase the train_ratio\")\n","else:\n","  print(\"Enough tokens for the training loader\")\n","\n","if total_tokens * (1 - train_ratio) < GPT_CONFIG_124M['context_length']:\n","  print(\"Not enough tokens for the validation loader, \"\n","  \"Try to lower the GPT_CONFIG_124M['context_length] or \"\n","  \"decrease the train_ratio\")\n","else:\n","  print(\"Enough tokens for the validation loader\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YpU2PFiJ_Pod","executionInfo":{"status":"ok","timestamp":1753259866688,"user_tz":-330,"elapsed":17,"user":{"displayName":"Dayanand K","userId":"10597641448182465017"}},"outputId":"f6e8f41f-c62c-4c40-c384-240a310dcf98"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Enough tokens for the training loader\n","Enough tokens for the validation loader\n"]}]},{"cell_type":"code","source":["print('Train loader')\n","for x, y in train_loader:\n","  print(x.shape, y.shape)\n","\n","print(f\"Number of training set: {len(train_loader)}\")\n","\n","print('\\nValidation loader')\n","for x, y in val_loader:\n","  print(x.shape, y.shape)\n","\n","print(f\"Number of validation set: {len(val_loader)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EXqIiQ-dAIJx","executionInfo":{"status":"ok","timestamp":1753259866706,"user_tz":-330,"elapsed":17,"user":{"displayName":"Dayanand K","userId":"10597641448182465017"}},"outputId":"47165853-3f70-4904-e388-27834758bcb6"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Train loader\n","torch.Size([2, 256]) torch.Size([2, 256])\n","torch.Size([2, 256]) torch.Size([2, 256])\n","torch.Size([2, 256]) torch.Size([2, 256])\n","torch.Size([2, 256]) torch.Size([2, 256])\n","torch.Size([2, 256]) torch.Size([2, 256])\n","torch.Size([2, 256]) torch.Size([2, 256])\n","torch.Size([2, 256]) torch.Size([2, 256])\n","torch.Size([2, 256]) torch.Size([2, 256])\n","torch.Size([2, 256]) torch.Size([2, 256])\n","Number of training set: 9\n","\n","Validation loader\n","torch.Size([2, 256]) torch.Size([2, 256])\n","Number of validation set: 1\n"]}]},{"cell_type":"code","source":["train_tokens = 0\n","\n","for input_tokens, target_tokens in train_loader:\n","  train_tokens += input_tokens.numel()\n","\n","print(f\"Number of training tokens: {train_tokens}\")\n","\n","val_tokens = 0\n","\n","for input_tokens, target_tokens in val_loader:\n","  val_tokens += input_tokens.numel()\n","\n","print(f\"Number of validation tokens: {val_tokens}\")\n","print(f\"All tokens: {train_tokens + val_tokens}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VzGB7QG3AmcY","executionInfo":{"status":"ok","timestamp":1753259866733,"user_tz":-330,"elapsed":26,"user":{"displayName":"Dayanand K","userId":"10597641448182465017"}},"outputId":"24be4429-82a1-4fcf-a6bb-e2196c5af0f5"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of training tokens: 4608\n","Number of validation tokens: 512\n","All tokens: 5120\n"]}]},{"cell_type":"code","source":["from torch import nn\n","\n","class MultiHeadAttention(nn.Module):\n","    def __init__(self, d_in, d_out, context_length, dropout, n_heads, qkv_bias=False):\n","        super().__init__()\n","        assert (d_out % n_heads == 0), \\\n","            \"d_out must be divisible by num_heads\"\n","\n","        self.d_out = d_out\n","        self.n_heads = n_heads\n","        self.head_dim = d_out // n_heads # Reduce the projection dim to match desired output dim\n","\n","        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n","        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n","        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n","        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n","        self.dropout = nn.Dropout(dropout)\n","        self.register_buffer(\n","            \"mask\",\n","            torch.triu(torch.ones(context_length, context_length),\n","                       diagonal=1)\n","        )\n","\n","    def forward(self, x):\n","        b, num_tokens, d_in = x.shape\n","\n","        keys = self.W_key(x) # Shape: (b, num_tokens, d_out)\n","        queries = self.W_query(x)\n","        values = self.W_value(x)\n","\n","        # We implicitly split the matrix by adding a `num_heads` dimension\n","        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n","        keys = keys.view(b, num_tokens, self.n_heads, self.head_dim) # Changed from num_heads to n_heads\n","        values = values.view(b, num_tokens, self.n_heads, self.head_dim) # Changed from num_heads to n_heads\n","        queries = queries.view(b, num_tokens, self.n_heads, self.head_dim) # Changed from num_heads to n_heads\n","\n","        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n","        keys = keys.transpose(1, 2)\n","        queries = queries.transpose(1, 2)\n","        values = values.transpose(1, 2)\n","\n","        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n","        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n","\n","        # Original mask truncated to the number of tokens and converted to boolean\n","        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n","\n","        # Use the mask to fill attention scores\n","        attn_scores.masked_fill_(mask_bool, -torch.inf)\n","\n","        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n","        attn_weights = self.dropout(attn_weights)\n","\n","        # Shape: (b, num_tokens, num_heads, head_dim)\n","        context_vec = (attn_weights @ values).transpose(1, 2)\n","\n","        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n","        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n","        context_vec = self.out_proj(context_vec) # optional projection\n","\n","        return context_vec"],"metadata":{"id":"DPq_i07iCI0U","executionInfo":{"status":"ok","timestamp":1753259866770,"user_tz":-330,"elapsed":28,"user":{"displayName":"Dayanand K","userId":"10597641448182465017"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["class LayerNorm(nn.Module):\n","  def __init__(self, emd_dim):\n","    super().__init__()\n","    self.eps = 1e-5\n","    self.scale = nn.Parameter(torch.ones(emd_dim))\n","    self.shift = nn.Parameter(torch.zeros(emd_dim))\n","\n","  def forward(self, x):\n","    mean = x.mean(dim = -1, keepdim = True)\n","    var = x.var(dim = -1, keepdim = True, unbiased = False)\n","    norm_x = (x - mean) / torch.sqrt(var + self.eps)\n","    # The model might lose important representational flexibility\n","    # if every normalized vector is constrained to always have zero mean and unit variance.\n","    #scale (also called gamma) is a learnable parameter that rescales each feature.\n","    #shift (also called beta) is a learnable parameter that shifts each feature.\n","    return self.scale * norm_x + self.shift\n","\n","class GELU(nn.Module):\n","  def __init__(self):\n","    super().__init__()\n","\n","  def forward(self, x):\n","    return 0.5 * x * (1 + torch.tanh(torch.sqrt(torch.tensor(2 / torch.pi)) * (x + 0.044715 * torch.pow(x, 3)))) # Converted torch.pi to tensor\n","\n","class FeedForward(nn.Module):\n","  def __init__(self, cfg):\n","    super().__init__()\n","    self.layers = nn.Sequential(\n","        nn.Linear(cfg['emd_dim'], 4 * cfg['emd_dim']),\n","        GELU(),\n","        nn.Linear(4 * cfg['emd_dim'], cfg['emd_dim'])\n","    )\n","\n","  def forward(self, x):\n","    return self.layers(x)"],"metadata":{"id":"MKoEm80LDJl6","executionInfo":{"status":"ok","timestamp":1753259866788,"user_tz":-330,"elapsed":17,"user":{"displayName":"Dayanand K","userId":"10597641448182465017"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["class DeepNeuralNetwork(nn.Module):\n","    def __init__(self, layer_sizes, use_shortcut):\n","        super().__init__()\n","        self.use_shortcut = use_shortcut\n","        self.layers = nn.ModuleList([\n","            nn.Sequential(nn.Linear(layer_sizes[0], layer_sizes[1]), GELU()),\n","            nn.Sequential(nn.Linear(layer_sizes[1], layer_sizes[2]), GELU()),\n","            nn.Sequential(nn.Linear(layer_sizes[2], layer_sizes[3]), GELU()),\n","            nn.Sequential(nn.Linear(layer_sizes[3], layer_sizes[4]), GELU()),\n","            nn.Sequential(nn.Linear(layer_sizes[4], layer_sizes[5]), GELU())\n","        ])\n","\n","    def forward(self, x):\n","        for layer in self.layers:\n","            # Compute the output of the current layer\n","            layer_output = layer(x)\n","            # Check if shortcut can be applied\n","            if self.use_shortcut and x.shape == layer_output.shape:\n","                x = x + layer_output\n","            else:\n","                x = layer_output\n","        return x\n"],"metadata":{"id":"BHcjUxyXDOD6","executionInfo":{"status":"ok","timestamp":1753259866808,"user_tz":-330,"elapsed":20,"user":{"displayName":"Dayanand K","userId":"10597641448182465017"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["class TransformerBlock(nn.Module):\n","  def __init__(self, cfg):\n","    super().__init__()\n","    self.att = MultiHeadAttention(\n","        d_in = cfg['emd_dim'],\n","        d_out = cfg['emd_dim'],\n","        # embed_dim = cfg['emd_dim'],\n","        context_length = cfg['context_length'],\n","        n_heads = cfg['n_heads'],\n","        dropout = cfg['drop_rate'], # Changed from drop_rate to dropout\n","        qkv_bias = cfg['qkv_bias'],\n","        # batch_first= True, # Removed batch_first as it's not a parameter in MultiHeadAttention\n","    )\n","    self.ff = FeedForward(cfg)\n","    self.ln1 = LayerNorm(cfg['emd_dim'])\n","    self.ln2 = LayerNorm(cfg['emd_dim'])\n","    self.drop_shortcut = nn.Dropout(cfg['drop_rate'])\n","\n","  def forward(self, x):\n","    shortcut = x\n","    x = self.ln1(x)\n","    x= self.att(x) # batch, n_tokens, emd_dim\n","    x = self.drop_shortcut(x)\n","    x = x + shortcut # add the original input back\n","\n","    shortcut = x\n","    x = self.ln2(x)\n","    x = self.ff(x)\n","    x = self.drop_shortcut(x)\n","    x = x + shortcut\n","\n","    return x"],"metadata":{"id":"rWjlea35DVRW","executionInfo":{"status":"ok","timestamp":1753259866839,"user_tz":-330,"elapsed":21,"user":{"displayName":"Dayanand K","userId":"10597641448182465017"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["# whole GPT-2 model\n","\n","class GPTModel(nn.Module):\n","  def __init__(self, cfg):\n","    super().__init__()\n","    self.tok_emb = nn.Embedding(cfg['vocab_size'], cfg['emd_dim'])\n","    self.pos_emb = nn.Embedding(cfg['context_length'], cfg['emd_dim'])\n","    self.drop_emb = nn.Dropout(cfg['drop_rate'])\n","\n","    self.trf_blocks = nn.Sequential(\n","        *[TransformerBlock(cfg) for _ in range(cfg['n_layers'])]\n","        )\n","\n","    self.final_norm = LayerNorm(cfg['emd_dim'])\n","\n","    self.out_head = nn.Linear(\n","        cfg['emd_dim'],\n","        cfg['vocab_size'],\n","        bias = False\n","    )\n","\n","  def forward(self, in_idx):\n","    batch_size, seq_len = in_idx.shape\n","    tok_embeds = self.tok_emb(in_idx)\n","    pos_embeds = self.pos_emb(torch.arange(seq_len, device = in_idx.device))\n","    x = tok_embeds + pos_embeds # batch, token , emb_dim\n","    x = self.drop_emb(x)\n","    x = self.trf_blocks(x) # Corrected attribute name\n","    x = self.final_norm(x)\n","    logits = self.out_head(x)\n","    return logits\n","\n"],"metadata":{"id":"lkiuv_bgDYbn","executionInfo":{"status":"ok","timestamp":1753259866852,"user_tz":-330,"elapsed":15,"user":{"displayName":"Dayanand K","userId":"10597641448182465017"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["torch.manual_seed(123)\n","model = GPTModel(GPT_CONFIG_124M)\n","model.eval()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_9xQUcckDcFg","executionInfo":{"status":"ok","timestamp":1753259868452,"user_tz":-330,"elapsed":1602,"user":{"displayName":"Dayanand K","userId":"10597641448182465017"}},"outputId":"94c9e4c4-942d-4cf2-eba2-49cc9fa4cf75"},"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/plain":["GPTModel(\n","  (tok_emb): Embedding(50257, 768)\n","  (pos_emb): Embedding(256, 768)\n","  (drop_emb): Dropout(p=0.1, inplace=False)\n","  (trf_blocks): Sequential(\n","    (0): TransformerBlock(\n","      (att): MultiHeadAttention(\n","        (W_query): Linear(in_features=768, out_features=768, bias=False)\n","        (W_key): Linear(in_features=768, out_features=768, bias=False)\n","        (W_value): Linear(in_features=768, out_features=768, bias=False)\n","        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU()\n","          (2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (ln1): LayerNorm()\n","      (ln2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.1, inplace=False)\n","    )\n","    (1): TransformerBlock(\n","      (att): MultiHeadAttention(\n","        (W_query): Linear(in_features=768, out_features=768, bias=False)\n","        (W_key): Linear(in_features=768, out_features=768, bias=False)\n","        (W_value): Linear(in_features=768, out_features=768, bias=False)\n","        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU()\n","          (2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (ln1): LayerNorm()\n","      (ln2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.1, inplace=False)\n","    )\n","    (2): TransformerBlock(\n","      (att): MultiHeadAttention(\n","        (W_query): Linear(in_features=768, out_features=768, bias=False)\n","        (W_key): Linear(in_features=768, out_features=768, bias=False)\n","        (W_value): Linear(in_features=768, out_features=768, bias=False)\n","        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU()\n","          (2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (ln1): LayerNorm()\n","      (ln2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.1, inplace=False)\n","    )\n","    (3): TransformerBlock(\n","      (att): MultiHeadAttention(\n","        (W_query): Linear(in_features=768, out_features=768, bias=False)\n","        (W_key): Linear(in_features=768, out_features=768, bias=False)\n","        (W_value): Linear(in_features=768, out_features=768, bias=False)\n","        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU()\n","          (2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (ln1): LayerNorm()\n","      (ln2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.1, inplace=False)\n","    )\n","    (4): TransformerBlock(\n","      (att): MultiHeadAttention(\n","        (W_query): Linear(in_features=768, out_features=768, bias=False)\n","        (W_key): Linear(in_features=768, out_features=768, bias=False)\n","        (W_value): Linear(in_features=768, out_features=768, bias=False)\n","        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU()\n","          (2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (ln1): LayerNorm()\n","      (ln2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.1, inplace=False)\n","    )\n","    (5): TransformerBlock(\n","      (att): MultiHeadAttention(\n","        (W_query): Linear(in_features=768, out_features=768, bias=False)\n","        (W_key): Linear(in_features=768, out_features=768, bias=False)\n","        (W_value): Linear(in_features=768, out_features=768, bias=False)\n","        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU()\n","          (2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (ln1): LayerNorm()\n","      (ln2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.1, inplace=False)\n","    )\n","    (6): TransformerBlock(\n","      (att): MultiHeadAttention(\n","        (W_query): Linear(in_features=768, out_features=768, bias=False)\n","        (W_key): Linear(in_features=768, out_features=768, bias=False)\n","        (W_value): Linear(in_features=768, out_features=768, bias=False)\n","        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU()\n","          (2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (ln1): LayerNorm()\n","      (ln2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.1, inplace=False)\n","    )\n","    (7): TransformerBlock(\n","      (att): MultiHeadAttention(\n","        (W_query): Linear(in_features=768, out_features=768, bias=False)\n","        (W_key): Linear(in_features=768, out_features=768, bias=False)\n","        (W_value): Linear(in_features=768, out_features=768, bias=False)\n","        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU()\n","          (2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (ln1): LayerNorm()\n","      (ln2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.1, inplace=False)\n","    )\n","    (8): TransformerBlock(\n","      (att): MultiHeadAttention(\n","        (W_query): Linear(in_features=768, out_features=768, bias=False)\n","        (W_key): Linear(in_features=768, out_features=768, bias=False)\n","        (W_value): Linear(in_features=768, out_features=768, bias=False)\n","        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU()\n","          (2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (ln1): LayerNorm()\n","      (ln2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.1, inplace=False)\n","    )\n","    (9): TransformerBlock(\n","      (att): MultiHeadAttention(\n","        (W_query): Linear(in_features=768, out_features=768, bias=False)\n","        (W_key): Linear(in_features=768, out_features=768, bias=False)\n","        (W_value): Linear(in_features=768, out_features=768, bias=False)\n","        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU()\n","          (2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (ln1): LayerNorm()\n","      (ln2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.1, inplace=False)\n","    )\n","    (10): TransformerBlock(\n","      (att): MultiHeadAttention(\n","        (W_query): Linear(in_features=768, out_features=768, bias=False)\n","        (W_key): Linear(in_features=768, out_features=768, bias=False)\n","        (W_value): Linear(in_features=768, out_features=768, bias=False)\n","        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU()\n","          (2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (ln1): LayerNorm()\n","      (ln2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.1, inplace=False)\n","    )\n","    (11): TransformerBlock(\n","      (att): MultiHeadAttention(\n","        (W_query): Linear(in_features=768, out_features=768, bias=False)\n","        (W_key): Linear(in_features=768, out_features=768, bias=False)\n","        (W_value): Linear(in_features=768, out_features=768, bias=False)\n","        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (ff): FeedForward(\n","        (layers): Sequential(\n","          (0): Linear(in_features=768, out_features=3072, bias=True)\n","          (1): GELU()\n","          (2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (ln1): LayerNorm()\n","      (ln2): LayerNorm()\n","      (drop_shortcut): Dropout(p=0.1, inplace=False)\n","    )\n","  )\n","  (final_norm): LayerNorm()\n","  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",")"]},"metadata":{},"execution_count":20}]},{"cell_type":"code","source":["# loss funtion\n","\n","def cal_loss_batch(input_batch, target_batch, model, device):\n","  input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n","  logits = model(input_batch)\n","  loss = nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n","  return loss\n","\n","def cal_loss_loader(data_loader, model, device, num_batches = None):\n","  total_loss = 0\n","  if len(data_loader) == 0:\n","    return float('nan')\n","  elif num_batches is None:\n","    num_batches = len(data_loader)\n","  else:\n","    num_batches = min(num_batches, len(data_loader))\n","\n","  for i, (input_batch, target_batch) in enumerate(data_loader):\n","    if i < num_batches:\n","      loss = cal_loss_batch(input_batch, target_batch, model, device)\n","      total_loss += loss.item()\n","    else:\n","      break\n","  return total_loss / num_batches\n"],"metadata":{"id":"R04s643gD3Ls","executionInfo":{"status":"ok","timestamp":1753259868454,"user_tz":-330,"elapsed":15,"user":{"displayName":"Dayanand K","userId":"10597641448182465017"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","device"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2ZP_TYioF8BL","executionInfo":{"status":"ok","timestamp":1753259868458,"user_tz":-330,"elapsed":6,"user":{"displayName":"Dayanand K","userId":"10597641448182465017"}},"outputId":"d9d64419-7b63-4a5f-db37-881a2b516120"},"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cpu')"]},"metadata":{},"execution_count":22}]},{"cell_type":"code","source":["model.to(device)\n","\n","torch.manual_seed(123)\n","\n","with torch.no_grad():\n","  train_loss = cal_loss_loader(train_loader, model, device)\n","  val_loss = cal_loss_loader(val_loader, model, device)\n","\n","print(f\"Train loss: {train_loss:.4f}\")\n","print(f\"Validation loss: {val_loss:.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZBeq3snVG5w_","executionInfo":{"status":"ok","timestamp":1753259899841,"user_tz":-330,"elapsed":31382,"user":{"displayName":"Dayanand K","userId":"10597641448182465017"}},"outputId":"7d6fe9f5-5333-4c5a-9a30-7f966841959a"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["Train loss: 10.9876\n","Validation loss: 10.9811\n"]}]},{"cell_type":"code","source":["perplexity = torch.exp(torch.tensor(train_loss))\n","perplexity"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YWc0TH_xHP2g","executionInfo":{"status":"ok","timestamp":1753259954199,"user_tz":-330,"elapsed":6,"user":{"displayName":"Dayanand K","userId":"10597641448182465017"}},"outputId":"5207d6fb-f938-4c64-b509-21a8057fbda7"},"execution_count":26,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(59135.2930)"]},"metadata":{},"execution_count":26}]},{"cell_type":"code","source":["def generate_text_simple(model, idx, max_new_tokens, context_size):\n","    # idx is (batch, n_tokens) array of indices in the current context\n","\n","    ###Input batch:\n"," ###tensor([[6109, 3626, 6100,  345],\n","        ##[6109, 1110, 6622,  257]])\n","\n","    for _ in range(max_new_tokens):\n","\n","        # Crop current context if it exceeds the supported context size\n","        # E.g., if LLM supports only 5 tokens, and the context size is 10\n","        # then only the last 5 tokens are used as context\n","        idx_cond = idx[:, -context_size:]\n","\n","        # Get the predictions\n","        with torch.no_grad():\n","            logits = model(idx_cond) ### batch, n_tokens, vocab_size\n","\n","        # Focus only on the last time step\n","        # (batch, n_tokens, vocab_size) becomes (batch, vocab_size)\n","        logits = logits[:, -1, :]\n","\n","        # Apply softmax to get probabilities\n","        probas = torch.softmax(logits, dim=-1)  # (batch, vocab_size)\n","\n","        # Get the idx of the vocab entry with the highest probability value\n","        idx_next = torch.argmax(probas, dim=-1, keepdim=True)  # (batch, 1)\n","\n","        # Append sampled index to the running sequence\n","        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n","\n","    return idx"],"metadata":{"id":"tSVPFC9GnkG4","executionInfo":{"status":"ok","timestamp":1753263001357,"user_tz":-330,"elapsed":46,"user":{"displayName":"Dayanand K","userId":"10597641448182465017"}}},"execution_count":33,"outputs":[]},{"cell_type":"code","source":["import tiktoken\n","\n","def text_to_token_ids(text, tokenizer):\n","    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n","    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n","    return encoded_tensor\n","\n","def token_ids_to_text(token_ids, tokenizer):\n","    flat = token_ids.squeeze(0) # remove batch dimension\n","    return tokenizer.decode(flat.tolist())\n","\n","start_context = \"Every effort moves you\"\n","\n","\n","\n","token_ids = generate_text_simple(\n","    model=model,\n","    idx=text_to_token_ids(start_context, tokenizer),\n","    max_new_tokens=10,\n","    context_size=GPT_CONFIG_124M[\"context_length\"]\n",")\n","\n","print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CYgBnjyqnABP","executionInfo":{"status":"ok","timestamp":1753263248413,"user_tz":-330,"elapsed":1833,"user":{"displayName":"Dayanand K","userId":"10597641448182465017"}},"outputId":"9947cea1-31ad-4937-b1e5-dbb9960d7888"},"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["Output text:\n"," Every effort moves you rentingetic wasnÙ… refres RexMeCHicular stren\n"]}]},{"cell_type":"code","source":["\n","# pretraining loop of the GPT model"],"metadata":{"id":"9rvKnY8-Hh0b","executionInfo":{"status":"ok","timestamp":1753261606423,"user_tz":-330,"elapsed":6,"user":{"displayName":"Dayanand K","userId":"10597641448182465017"}}},"execution_count":27,"outputs":[]},{"cell_type":"code","source":["def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n","  model.eval()\n","\n","  with torch.no_grad():\n","    train_loss = cal_loss_loader(train_loader, model, device,num_batches = eval_iter)\n","    val_loss = cal_loss_loader(val_loader, model, device, num_batches = eval_iter)\n","\n","  model.train()\n","\n","  return train_loss, val_loss"],"metadata":{"id":"jM00DXFllcUA","executionInfo":{"status":"ok","timestamp":1753262921303,"user_tz":-330,"elapsed":11,"user":{"displayName":"Dayanand K","userId":"10597641448182465017"}}},"execution_count":32,"outputs":[]},{"cell_type":"code","source":["def generate_and_print_sample(model, tokenizer, device, start_context):\n","  model.eval()\n","  context_size = model.pos_emb.weight.shape[0]\n","  encoded = text_to_token_ids(start_context, tokenizer).to(device)\n","\n","  with torch.no_grad():\n","    token_ids = generate_text_simple(\n","        model= model, idx = encoded,\n","        max_new_tokens = 50, context_size = context_size\n","    )\n","\n","  decoded_text = token_ids_to_text(token_ids, tokenizer)\n","  print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format\n","  model.train()"],"metadata":{"id":"aJhGXvg7lcoQ","executionInfo":{"status":"ok","timestamp":1753263267495,"user_tz":-330,"elapsed":6,"user":{"displayName":"Dayanand K","userId":"10597641448182465017"}}},"execution_count":36,"outputs":[]},{"cell_type":"code","source":["def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n","                       eval_freq, eval_iter, start_context, tokenizer):\n","\n","  train_losses, val_losses, track_tokens_seen = [], [], []\n","  tokens_seen, global_step = 0, -1\n","\n","  for epoch in range(num_epochs):\n","    model.train()\n","\n","    for input_batch, target_batch in train_loader:\n","      optimizer.zero_grad()\n","      loss = cal_loss_batch(input_batch, target_batch, model, device)\n","      loss.backward()\n","      optimizer.step()\n","      tokens_seen += input_batch.numel()\n","      global_step += 1\n","\n","      if global_step % eval_freq == 0:\n","        train_loss, val_loss = evaluate_model(\n","            model, train_loader, val_loader, device, eval_iter\n","        )\n","        train_losses.append(train_loss)\n","        val_losses.append(val_loss)\n","        track_tokens_seen.append(tokens_seen)\n","        print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n","              f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n","\n","\n","    generate_and_print_sample(\n","        model, tokenizer, device, start_context\n","    )\n","  return train_losses, val_losses, track_tokens_seen"],"metadata":{"id":"kcEFl0GFiQPL","executionInfo":{"status":"ok","timestamp":1753263270549,"user_tz":-330,"elapsed":51,"user":{"displayName":"Dayanand K","userId":"10597641448182465017"}}},"execution_count":37,"outputs":[]},{"cell_type":"code","source":["# Note:\n","# Uncomment the following code to calculate the execution time\n","import time\n","start_time = time.time()\n","\n","torch.manual_seed(123)\n","model = GPTModel(GPT_CONFIG_124M)\n","model.to(device)\n","optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n","\n","num_epochs = 10\n","train_losses, val_losses, tokens_seen = train_model_simple(\n","    model, train_loader, val_loader, optimizer, device,\n","    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n","    start_context=\"Every effort moves you\", tokenizer=tokenizer\n",")\n","\n","# Note:\n","# Uncomment the following code to show the execution time\n","end_time = time.time()\n","execution_time_minutes = (end_time - start_time) / 60\n","print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mY4NJ-3zomjj","executionInfo":{"status":"ok","timestamp":1753264780443,"user_tz":-330,"elapsed":1462058,"user":{"displayName":"Dayanand K","userId":"10597641448182465017"}},"outputId":"e31e753f-1cb6-4757-c96d-1fb13ce3fa0b"},"execution_count":38,"outputs":[{"output_type":"stream","name":"stdout","text":["Ep 1 (Step 000000): Train loss 9.783, Val loss 9.927\n","Ep 1 (Step 000005): Train loss 7.985, Val loss 8.335\n","Every effort moves you,,,,,,,,,,,,.                                     \n","Ep 2 (Step 000010): Train loss 6.753, Val loss 7.048\n","Ep 2 (Step 000015): Train loss 6.114, Val loss 6.573\n","Every effort moves you, and,, and, and,,,,, and, and,,,,,,,,,,,,,, and,,,, and,, and,,,,, and,,,,,,\n","Ep 3 (Step 000020): Train loss 5.525, Val loss 6.490\n","Ep 3 (Step 000025): Train loss 5.324, Val loss 6.387\n","Every effort moves you, and to the picture.                      \"I, and the of the of the's the honour, and, and I had been, and I\n","Ep 4 (Step 000030): Train loss 4.761, Val loss 6.360\n","Ep 4 (Step 000035): Train loss 4.461, Val loss 6.258\n","Every effort moves you of the to the picture--as of the picture--as I had been \" it was his \" I was the     \"I was his I had been the his pictures--and it the picture and I had been the picture of\n","Ep 5 (Step 000040): Train loss 3.833, Val loss 6.196\n","Every effort moves you know the \"Oh, and he was not the fact by his last word.         \"I was.      \"Oh, I felt a little a little the    \n","Ep 6 (Step 000045): Train loss 3.352, Val loss 6.139\n","Ep 6 (Step 000050): Train loss 2.861, Val loss 6.112\n","Every effort moves you know; and my dear, and he was not the fact with a little of the house of the fact of the fact, and.                       \n","Ep 7 (Step 000055): Train loss 2.347, Val loss 6.138\n","Ep 7 (Step 000060): Train loss 2.084, Val loss 6.179\n","Every effort moves you know,\" was one of the picture for nothing--I told Mrs.  \"I looked--as of the fact, and I felt him--his back his head to the donkey. \"Oh, and_--because he had always _\n","Ep 8 (Step 000065): Train loss 1.521, Val loss 6.176\n","Ep 8 (Step 000070): Train loss 1.272, Val loss 6.178\n","Every effort moves you?\" \"I didn't bear the picture--I told me.  \"I looked up, and went on groping and Mrs. I was back the head to look up at the honour being _mine_--because he was when I\n","Ep 9 (Step 000075): Train loss 1.000, Val loss 6.277\n","Ep 9 (Step 000080): Train loss 0.718, Val loss 6.281\n","Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n","Ep 10 (Step 000085): Train loss 0.506, Val loss 6.325\n","Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to the donkey again. I saw that, and down the room, when I\n","Training completed in 24.37 minutes.\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"03eP-x8voyPm"},"execution_count":null,"outputs":[]}]}