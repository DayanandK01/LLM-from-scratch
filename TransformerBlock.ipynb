{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMUSK/HgYCkKioab7BqOrzJ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"nNwG7pYa2Yis","executionInfo":{"status":"ok","timestamp":1753160917419,"user_tz":-330,"elapsed":10148,"user":{"displayName":"Dayanand K","userId":"10597641448182465017"}}},"outputs":[],"source":["import torch\n","import re\n","import pandas as pd\n","import numpy as np\n","import importlib\n","import tiktoken\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader"]},{"cell_type":"markdown","source":["# Building a transformer block\n"],"metadata":{"id":"TdKmXMGh7-_m"}},{"cell_type":"code","source":["\n","\n","GPT_CONFIG_124M = {\n","    'vocab_size': 50257,\n","    'context_length': 1024,\n","    'emd_dim': 768,\n","    'n_heads': 12,\n","    'n_layers': 12,\n","    'drop_rate': 0.1,\n","    'qkv_bias': False\n","\n","}"],"metadata":{"id":"J9llcIP57ILF","executionInfo":{"status":"ok","timestamp":1753160917432,"user_tz":-330,"elapsed":23,"user":{"displayName":"Dayanand K","userId":"10597641448182465017"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["class MultiHeadAttention(nn.Module):\n","    def __init__(self, d_in, d_out, context_length, dropout, n_heads, qkv_bias=False):\n","        super().__init__()\n","        assert (d_out % n_heads == 0), \\\n","            \"d_out must be divisible by num_heads\"\n","\n","        self.d_out = d_out\n","        self.n_heads = n_heads\n","        self.head_dim = d_out // n_heads # Reduce the projection dim to match desired output dim\n","\n","        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n","        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n","        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n","        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n","        self.dropout = nn.Dropout(dropout)\n","        self.register_buffer(\n","            \"mask\",\n","            torch.triu(torch.ones(context_length, context_length),\n","                       diagonal=1)\n","        )\n","\n","    def forward(self, x):\n","        b, num_tokens, d_in = x.shape\n","\n","        keys = self.W_key(x) # Shape: (b, num_tokens, d_out)\n","        queries = self.W_query(x)\n","        values = self.W_value(x)\n","\n","        # We implicitly split the matrix by adding a `num_heads` dimension\n","        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n","        keys = keys.view(b, num_tokens, self.n_heads, self.head_dim) # Changed from num_heads to n_heads\n","        values = values.view(b, num_tokens, self.n_heads, self.head_dim) # Changed from num_heads to n_heads\n","        queries = queries.view(b, num_tokens, self.n_heads, self.head_dim) # Changed from num_heads to n_heads\n","\n","        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n","        keys = keys.transpose(1, 2)\n","        queries = queries.transpose(1, 2)\n","        values = values.transpose(1, 2)\n","\n","        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n","        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n","\n","        # Original mask truncated to the number of tokens and converted to boolean\n","        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n","\n","        # Use the mask to fill attention scores\n","        attn_scores.masked_fill_(mask_bool, -torch.inf)\n","\n","        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n","        attn_weights = self.dropout(attn_weights)\n","\n","        # Shape: (b, num_tokens, num_heads, head_dim)\n","        context_vec = (attn_weights @ values).transpose(1, 2)\n","\n","        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n","        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n","        context_vec = self.out_proj(context_vec) # optional projection\n","\n","        return context_vec"],"metadata":{"id":"xsBunUXcVZ-s","executionInfo":{"status":"ok","timestamp":1753160917451,"user_tz":-330,"elapsed":39,"user":{"displayName":"Dayanand K","userId":"10597641448182465017"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["The building blocks: Layer normalization, GELU AF and feed forward nn\n"],"metadata":{"id":"rI21s6fw7vNg"}},{"cell_type":"code","source":["class LayerNorm(nn.Module):\n","  def __init__(self, emd_dim):\n","    super().__init__()\n","    self.eps = 1e-5\n","    self.scale = nn.Parameter(torch.ones(emd_dim))\n","    self.shift = nn.Parameter(torch.zeros(emd_dim))\n","\n","  def forward(self, x):\n","    mean = x.mean(dim = -1, keepdim = True)\n","    var = x.var(dim = -1, keepdim = True, unbiased = False)\n","    norm_x = (x - mean) / torch.sqrt(var + self.eps)\n","    return self.scale * norm_x + self.shift\n","\n","class GELU(nn.Module):\n","  def __init__(self):\n","    super().__init__()\n","\n","  def forward(self, x):\n","    return 0.5 * x * (1 + torch.tanh(torch.sqrt(torch.tensor(2 / torch.pi)) * (x + 0.044715 * torch.pow(x, 3)))) # Converted torch.pi to tensor\n","\n","class FeedForward(nn.Module):\n","  def __init__(self, cfg):\n","    super().__init__()\n","    self.layers = nn.Sequential(\n","        nn.Linear(cfg['emd_dim'], 4 * cfg['emd_dim']),\n","        GELU(),\n","        nn.Linear(4 * cfg['emd_dim'], cfg['emd_dim'])\n","    )\n","\n","  def forward(self, x):\n","    return self.layers(x)"],"metadata":{"id":"rApaapHA79bE","executionInfo":{"status":"ok","timestamp":1753160917476,"user_tz":-330,"elapsed":24,"user":{"displayName":"Dayanand K","userId":"10597641448182465017"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["class ExampleDeepNeuralNetwork(nn.Module):\n","    def __init__(self, layer_sizes, use_shortcut):\n","        super().__init__()\n","        self.use_shortcut = use_shortcut\n","        self.layers = nn.ModuleList([\n","            nn.Sequential(nn.Linear(layer_sizes[0], layer_sizes[1]), GELU()),\n","            nn.Sequential(nn.Linear(layer_sizes[1], layer_sizes[2]), GELU()),\n","            nn.Sequential(nn.Linear(layer_sizes[2], layer_sizes[3]), GELU()),\n","            nn.Sequential(nn.Linear(layer_sizes[3], layer_sizes[4]), GELU()),\n","            nn.Sequential(nn.Linear(layer_sizes[4], layer_sizes[5]), GELU())\n","        ])\n","\n","    def forward(self, x):\n","        for layer in self.layers:\n","            # Compute the output of the current layer\n","            layer_output = layer(x)\n","            # Check if shortcut can be applied\n","            if self.use_shortcut and x.shape == layer_output.shape:\n","                x = x + layer_output\n","            else:\n","                x = layer_output\n","        return x\n"],"metadata":{"id":"GSr5YmEnfrQi","executionInfo":{"status":"ok","timestamp":1753160917557,"user_tz":-330,"elapsed":77,"user":{"displayName":"Dayanand K","userId":"10597641448182465017"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["# Transformer Block\n"],"metadata":{"id":"FB4pceN4ADzH"}},{"cell_type":"code","source":["class TransformerBlock(nn.Module):\n","  def __init__(self, cfg):\n","    super().__init__()\n","    self.att = MultiHeadAttention(\n","        d_in = cfg['emd_dim'],\n","        d_out = cfg['emd_dim'],\n","        # embed_dim = cfg['emd_dim'],\n","        context_length = cfg['context_length'],\n","        n_heads = cfg['n_heads'],\n","        dropout = cfg['drop_rate'], # Changed from drop_rate to dropout\n","        qkv_bias = cfg['qkv_bias'],\n","        # batch_first= True, # Removed batch_first as it's not a parameter in MultiHeadAttention\n","    )\n","    self.ff = FeedForward(cfg)\n","    self.ln1 = LayerNorm(cfg['emd_dim'])\n","    self.ln2 = LayerNorm(cfg['emd_dim'])\n","    self.drop_shortcut = nn.Dropout(cfg['drop_rate'])\n","\n","  def forward(self, x):\n","    shortcut = x\n","    x = self.ln1(x)\n","    x= self.att(x) # batch, n_tokens, emd_dim\n","    x = self.drop_shortcut(x)\n","    x = x + shortcut # add the original input back\n","\n","    shortcut = x\n","    x = self.ln2(x)\n","    x = self.ff(x)\n","    x = self.drop_shortcut(x)\n","    x = x + shortcut\n","\n","    return x"],"metadata":{"id":"Ql8OBjEs_X50","executionInfo":{"status":"ok","timestamp":1753160917674,"user_tz":-330,"elapsed":11,"user":{"displayName":"Dayanand K","userId":"10597641448182465017"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"C60RLib9Bp3C","executionInfo":{"status":"ok","timestamp":1753160917748,"user_tz":-330,"elapsed":73,"user":{"displayName":"Dayanand K","userId":"10597641448182465017"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# whole GPT-2 model\n","\n","class GPTModel(nn.Module):\n","  def __init__(self, cfg):\n","    super().__init__()\n","    self.tok_emb = nn.Embedding(cfg['vocab_size'], cfg['emd_dim'])\n","    self.pos_emb = nn.Embedding(cfg['context_length'], cfg['emd_dim'])\n","    self.drop_emb = nn.Dropout(cfg['drop_rate'])\n","\n","    self.trf_blocks = nn.Sequential(\n","        *[TransformerBlock(cfg) for _ in range(cfg['n_layers'])]\n","        )\n","\n","    self.final_norm = LayerNorm(cfg['emd_dim'])\n","\n","    self.out_head = nn.Linear(\n","        cfg['emd_dim'],\n","        cfg['vocab_size'],\n","        bias = False\n","    )\n","\n","  def forward(self, in_idx):\n","    batch_size, seq_len = in_idx.shape\n","    tok_embeds = self.tok_emb(in_idx)\n","    pos_embeds = self.pos_emb(torch.arange(seq_len, device = in_idx.device))\n","    x = tok_embeds + pos_embeds # batch, token , emb_dim\n","    x = self.drop_emb(x)\n","    x = self.trf_blocks(x) # Corrected attribute name\n","    x = self.final_norm(x)\n","    logits = self.out_head(x)\n","    return logits\n","\n"],"metadata":{"id":"LELc3JA7Dkd1","executionInfo":{"status":"ok","timestamp":1753160917859,"user_tz":-330,"elapsed":109,"user":{"displayName":"Dayanand K","userId":"10597641448182465017"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["import tiktoken\n","tokenizer = tiktoken.get_encoding(\"gpt2\")\n","batch = []\n","txt1 = \"Every effort moves you\"\n","txt2 = \"Every day holds a\"\n","batch.append(torch.tensor(tokenizer.encode(txt1)))\n","batch.append(torch.tensor(tokenizer.encode(txt2)))\n","batch = torch.stack(batch, dim=0)\n","print(batch)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VH1YsmthZk86","executionInfo":{"status":"ok","timestamp":1753160919385,"user_tz":-330,"elapsed":1528,"user":{"displayName":"Dayanand K","userId":"10597641448182465017"}},"outputId":"8f6dd66e-fa28-45e4-fe5f-643665425ee8"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[6109, 3626, 6100,  345],\n","        [6109, 1110, 6622,  257]])\n"]}]},{"cell_type":"code","source":["torch.manual_seed(123)\n","model = GPTModel(GPT_CONFIG_124M)\n","logits = model(batch)\n","print(\"Output shape:\", logits.shape)\n","print(logits)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xEclWHk9Zqxw","executionInfo":{"status":"ok","timestamp":1753160921375,"user_tz":-330,"elapsed":1992,"user":{"displayName":"Dayanand K","userId":"10597641448182465017"}},"outputId":"c14af348-c9f9-4545-bfe0-276b8d3b99c2"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Output shape: torch.Size([2, 4, 50257])\n","tensor([[[ 0.1381,  0.0077, -0.1963,  ..., -0.0222, -0.1060,  0.1717],\n","         [ 0.3865, -0.8408, -0.6564,  ..., -0.5163,  0.2369, -0.3357],\n","         [ 0.6989, -0.1829, -0.1631,  ...,  0.1472, -0.6504, -0.0056],\n","         [-0.4290,  0.1669, -0.1258,  ...,  1.1579,  0.5303, -0.5549]],\n","\n","        [[ 0.1094, -0.2894, -0.1467,  ..., -0.0557,  0.2911, -0.2824],\n","         [ 0.0882, -0.3552, -0.3527,  ...,  1.2930,  0.0053,  0.1898],\n","         [ 0.6091,  0.4702, -0.4094,  ...,  0.7688,  0.3787, -0.1974],\n","         [-0.0612, -0.0737,  0.4751,  ...,  1.2463, -0.3834,  0.0609]]],\n","       grad_fn=<UnsafeViewBackward0>)\n"]}]},{"cell_type":"code","source":["total_params =sum(p.numel() for p in model.parameters())\n","print(f\"Total number of parameters: {total_params:,}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2wuy9BebZyzT","executionInfo":{"status":"ok","timestamp":1753160921446,"user_tz":-330,"elapsed":70,"user":{"displayName":"Dayanand K","userId":"10597641448182465017"}},"outputId":"fb4e7ed0-5218-4452-c0ac-dc42a5b30c65"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Total number of parameters: 163,009,536\n"]}]},{"cell_type":"code","source":["print(\"Token embedding layer shape:\", model.tok_emb.weight.shape)\n","print(\"Output layer shape:\", model.out_head.weight.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VYiy8FewgAEu","executionInfo":{"status":"ok","timestamp":1753160921452,"user_tz":-330,"elapsed":5,"user":{"displayName":"Dayanand K","userId":"10597641448182465017"}},"outputId":"8ba7dec0-ff4b-4a56-d373-2fd08ca59b6e"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Token embedding layer shape: torch.Size([50257, 768])\n","Output layer shape: torch.Size([50257, 768])\n"]}]},{"cell_type":"code","source":["total_params_gpt2 = total_params - sum(p.numel() for p in model.out_head.parameters())\n","print(f\"Number of trainable parameters considering weight tying: {total_params_gpt2:,}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CIEfRSEVgEj6","executionInfo":{"status":"ok","timestamp":1753160921469,"user_tz":-330,"elapsed":15,"user":{"displayName":"Dayanand K","userId":"10597641448182465017"}},"outputId":"3dde5a46-921a-4135-d6de-bdfe2f3cb323"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of trainable parameters considering weight tying: 124,412,160\n"]}]},{"cell_type":"code","source":["total_size_bytes = total_params * 4 #A\n","total_size_mb = total_size_bytes / (1024 * 1024) #B\n","print(f\"Total size of the model: {total_size_mb:.2f} MB\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YqwKWzxegHNx","executionInfo":{"status":"ok","timestamp":1753160921469,"user_tz":-330,"elapsed":7,"user":{"displayName":"Dayanand K","userId":"10597641448182465017"}},"outputId":"63113f8e-7db9-4d6d-e577-a2d288fd9d25"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Total size of the model: 621.83 MB\n"]}]},{"cell_type":"code","source":["# predicting the next word in a sequence\n","\n","def generate_text_word(model, idx, max_new_tokens, context_size):\n","  # batch, tokens\n","  for _ in range(max_new_tokens):\n","\n","    print('idx' , idx.shape)\n","    idx_cond = idx[:, -context_size:]\n","    print('idx_cond' , idx_cond.shape)\n","\n","    with torch.no_grad():\n","      logits = model(idx_cond)\n","\n","    print('logits', logits.shape)\n","    logits = logits[:, -1, :]\n","\n","    probas = torch.softmax(logits, dim=-1)\n","    print('probas', probas.shape)\n","\n","    idx_next = torch.argmax(probas, dim = -1, keepdim = True)\n","\n","    idx = torch.cat((idx, idx_next), dim = 1)\n","    print('final idx', idx.shape)\n","\n","  return idx"],"metadata":{"id":"ptwvsO3kgLFI","executionInfo":{"status":"ok","timestamp":1753164225980,"user_tz":-330,"elapsed":6,"user":{"displayName":"Dayanand K","userId":"10597641448182465017"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["start_context = \"Hello, I am\"\n","encoded = tokenizer.encode(start_context)\n","print(\"encoded:\", encoded)\n","encoded_tensor = torch.tensor(encoded).unsqueeze(0) #A\n","print(\"encoded_tensor.shape:\", encoded_tensor.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tbNY0lgUoT1k","executionInfo":{"status":"ok","timestamp":1753164227032,"user_tz":-330,"elapsed":21,"user":{"displayName":"Dayanand K","userId":"10597641448182465017"}},"outputId":"e4a11743-5f10-4824-c9d1-1e708ed9a66a"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["encoded: [15496, 11, 314, 716]\n","encoded_tensor.shape: torch.Size([1, 4])\n"]}]},{"cell_type":"code","source":["model.eval() #A\n","out = generate_text_word(\n","model=model,\n","idx=encoded_tensor,\n","max_new_tokens=6,\n","context_size=GPT_CONFIG_124M[\"context_length\"]\n",")\n","print(\"Output:\", out)\n","print(\"Output length:\", len(out[0]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mTfo9lNQtMan","executionInfo":{"status":"ok","timestamp":1753164229111,"user_tz":-330,"elapsed":943,"user":{"displayName":"Dayanand K","userId":"10597641448182465017"}},"outputId":"fedcf85c-3ce5-43f7-ec8c-cf4dce349398"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["idx torch.Size([1, 4])\n","idx_cond torch.Size([1, 4])\n","logits torch.Size([1, 4, 50257])\n","probas torch.Size([1, 50257])\n","final idx torch.Size([1, 5])\n","idx torch.Size([1, 5])\n","idx_cond torch.Size([1, 5])\n","logits torch.Size([1, 5, 50257])\n","probas torch.Size([1, 50257])\n","final idx torch.Size([1, 6])\n","idx torch.Size([1, 6])\n","idx_cond torch.Size([1, 6])\n","logits torch.Size([1, 6, 50257])\n","probas torch.Size([1, 50257])\n","final idx torch.Size([1, 7])\n","idx torch.Size([1, 7])\n","idx_cond torch.Size([1, 7])\n","logits torch.Size([1, 7, 50257])\n","probas torch.Size([1, 50257])\n","final idx torch.Size([1, 8])\n","idx torch.Size([1, 8])\n","idx_cond torch.Size([1, 8])\n","logits torch.Size([1, 8, 50257])\n","probas torch.Size([1, 50257])\n","final idx torch.Size([1, 9])\n","idx torch.Size([1, 9])\n","idx_cond torch.Size([1, 9])\n","logits torch.Size([1, 9, 50257])\n","probas torch.Size([1, 50257])\n","final idx torch.Size([1, 10])\n","Output: tensor([[15496,    11,   314,   716, 27018, 24086, 47843, 30961, 42348,  7267]])\n","Output length: 10\n"]}]},{"cell_type":"code","source":["decoded_text = tokenizer.decode(out.squeeze(0).tolist())\n","print(decoded_text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3977gXsutQFK","executionInfo":{"status":"ok","timestamp":1753163860450,"user_tz":-330,"elapsed":7,"user":{"displayName":"Dayanand K","userId":"10597641448182465017"}},"outputId":"ba3eb332-7f70-4dae-9637-31a55804a8fd"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["Hello, I am Featureiman Byeswickattribute argue\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"xJwb8FsAtYi5"},"execution_count":null,"outputs":[]}]}